{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Frequency Trading Algorithm\n",
    "\n",
    "You have been tasked by the investment firm Renaissance High Frequency Trading (RHFT) to develop an automated trading strategy utilizing a combination of machine learning algorithms and high frequency algorithms. RHFT wants this new algorithm to be based on stock market data of the 30 stocks in the Dow Jones at the minute level and to conduct buys and sells every minute based on 1 min, 5 min, and 10 min Momentum. The CIO asked you to choose the Machine Learning Algorithm best suited for this task and wants you to execute the trades via Alpaca's API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pytz\n",
    "from dotenv import load_dotenv\n",
    "import schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Alpaca API object, specifying use of the paper trading account:\n",
    "api = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    base_url = 'https://paper-api.alpaca.markets',\n",
    "    api_version = \"v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and check account information\n",
    "account = api.get_account()\n",
    "print(account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a ticker list, beginning and end dates, and timeframe interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of tickers\n",
    "ticker_list = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOGL\", \"MSFT\",\"TSLA\"]\n",
    "# declare begin and end date strings\n",
    "beg_date = '2021-01-05'\n",
    "end_date = '2021-01-05'\n",
    "# we convert begin and end date to formats that the ALPACA API requires\n",
    "start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "end   =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=16, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "# We set the time frequency at which we want to pull prices\n",
    "timeframe='1Min'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ping the Alpaca API for the data and store it in a DataFrame called `prices` by using the `get_barset` function combined with the `df` method from the Alpaca Trade SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull prices from the ALPACA API\n",
    "prices = api.get_barset(ticker_list, timeframe,limit=1000, start=start, end=end).df\n",
    "prices.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Store only the close prices from the `prices` DataFrame in a new DataFrame called `df_closing_prices`, then view the head and tail to confirm the following:\n",
    "* First price for each stock on the open at 9:30 Eastern Time.\n",
    "* Last price for the day on the close at 3:59 pm Eastern Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the closing prices for each one of the tickers and store in a column in df_closing_prices amed after that ticker\n",
    "df_closing_prices = pd.DataFrame({\n",
    "    \"FB\": prices[\"FB\"].close,\n",
    "    \"AMZN\": prices[\"AMZN\"].close,\n",
    "    \"AAPL\": prices[\"AAPL\"].close,\n",
    "    \"NFLX\": prices[\"NFLX\"].close,\n",
    "    \"GOOGL\": prices[\"GOOGL\"].close,\n",
    "    \"MSFT\": prices[\"MSFT\"].close,\n",
    "    \"TSLA\": prices[\"TSLA\"].close,\n",
    "    }, index=prices.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first five rows\n",
    "df_closing_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview last five rows\n",
    "df_closing_prices.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. When viewing the head and tail, you'll notice several `NaN` values.\n",
    "* Alpaca reports `NaN` for minutes without any trades occuring as missing.\n",
    "* These values must be removed, we use Panda's `ffill()` function to \"forward fill\", or replace, those prices with the previous values (since the price has not changed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas' forward fill function to fill missing values (be sure to set inplace=True)\n",
    "df_closing_prices.ffill(inplace=True)\n",
    "df_closing_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compute the percentage change values for 1 minute as follows:\n",
    "* Create a variable called `forecast` to hold the forecast, in this case `1` for 1 minute.\n",
    "* Use the `pct_change` function, passing in the `forecast`, on the `df_closing_prices` DataFrame, storeing the newly generated DataFrame in a variable called `returns`.\n",
    "* Convert the `returns` DataFrame to show forward returns by passing `-(forecast)` into the `shift function.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable to set prediction period\n",
    "forecast = 1\n",
    "\n",
    "# Compute the pct_change for 1 min \n",
    "returns = df_closing_prices.pct_change(periods=forecast)\n",
    "\n",
    "# Shift the returns to convert them to forward returns\n",
    "returns = returns.shift(-(forecast))\n",
    "\n",
    "# Preview the DataFrame\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "> You can verify these returns are computed correctly by analyzing the first observation for Facebook:\n",
    "> * 9:30 am for 0.000632.\n",
    " \n",
    "> How is that number computed? \n",
    " \n",
    "> * The price of Facebook at 9:30 is 269.00\n",
    "> * The price of Facebook at 9:31 is 269.17\n",
    "\n",
    "> Which gives you:\n",
    "\n",
    "> * (269.17 - \t269.00)/ 269.90 = 0.000632\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert the DataFrame into long form for merging later using `unstack` and `reset_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unstack() to bring the data in long format and save the output as as dataframe\n",
    "returns = pd.DataFrame(returns.unstack(level=0))\n",
    "\n",
    "# Rename the column to make it easer to identify it:\n",
    "name = f'F_{forecast}_m_returns'\n",
    "returns.rename(columns={0: name}, inplace = True)\n",
    "\n",
    "# Reset the index of the dataframe for merging later (be sure to set inplace=True)\n",
    "returns.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five rows\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the last five rows\n",
    "returns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute the 1, 5, 10 minute momentums that will be used to predict the forward returns, then merge them with the forward returns as follows:\n",
    "* Create the list of moments: `list_of_momentums = [1,5,10]`.\n",
    "* Write a for-loop to loop through the `list_of_momentums`, applying them to `pct_change` with the `df_closing_price` with each iteration.\n",
    "* With each loop, the data temporary DataFrame, `returns_temp` will need to be prepped with `unstack` and `reset_index`, then added as a new column to the original `returns` DataFrame from the prior step.\n",
    "* Complete this step by dropping the null values from `returns` and creating a multi-index based on date and ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of momentums that we want to predict\n",
    "list_of_momentums = [1,5,10]\n",
    "for i in list_of_momentums:   \n",
    "    # Compute percentage change for each one of the momentums in the momentum list\n",
    "    pct_chg = df_closing_prices.pct_change(i)\n",
    "    \n",
    "    # Unstack the returns and save the output as as dataframe called returns_temp \n",
    "    returns_temp = pd.DataFrame(pct_chg.unstack(level=0))\n",
    "    \n",
    "    # Rename the column to make it easer to identify it:\n",
    "    name = f'{i}_m_returns'\n",
    "    returns_temp.rename(columns={0: name}, inplace = True)\n",
    "    \n",
    "    # Reset the index so we can merge based on index\n",
    "    returns_temp.reset_index(inplace=True)\n",
    "    \n",
    "    # Merge returns_temp  with the original returns \n",
    "    returns = pd.merge(returns,returns_temp,left_on=['level_0', 'time'],right_on=['level_0', 'time'], how='left', suffixes=('_original', 'right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dropna() to get rid of those missing observations.\n",
    "returns.dropna(inplace=True)\n",
    "\n",
    "# Create a multi index based on level_0 and time\n",
    "returns.set_index(['level_0','time'], inplace=True)\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train and Compare Multiple Machine Learning Algorithms\n",
    "\n",
    " In this section, you'll train each of the requested algorithms and compare performance. Be sure to use the same parameters and training steps for each model. This is necessary to compare each model accurately.\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "#### 1. Generate your feature data (`X`) and target data (`y`):\n",
    "* Create a dataframe `X` that contains all the columns from the returns dataframe that will be used to predict `F_1_m_returns`.\n",
    "* Create a variable, called `y`, that is equal 1 if `F_1_m_returns` is larger than 0. This will be our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe for features and define the target variable as a binary target\n",
    "X = returns.iloc[:,1:4]\n",
    "\n",
    "# Create the target variable\n",
    "y = []\n",
    "\n",
    "# Loop through the returns[\"F_1_m_returns\"] data and append 0 or 1 to y based on returns\n",
    "for row in returns[\"F_1_m_returns\"]:\n",
    "    if row > 0:\n",
    "        y.append(1)\n",
    "\n",
    "    elif row <= 0:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.head()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "> Notice that we don't use shuffle when splitting the dataset into a training and testing dataset. \n",
    "\n",
    "> We want to keep the original ordering of the data, so we don't end up using observations in the future to predict past observations,\n",
    "\n",
    "> This is a critical mistake known as look ahead bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use the train_test_split library to split the dataset into a training and testing dataset, with 70% used for testing\n",
    "* Set the shuffle parameter to False, so that you use the first 70% for training to prvent look ahead bias.\n",
    "* Make sure you have these 4 variables: `X_train`, `X_test`, `y_train`, `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset without shuffling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the `Counter` function to test the distribution of the data. \n",
    "* The result of `Counter({1: 668, 0: 1194})` reveals the data is indeed unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter function from the collections library\n",
    "from collections import Counter\n",
    "\n",
    "# Use Counter to count the number 1s and 0 in y_train\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balance the dataset with the Oversampler libary, setting `random state= 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomOverSampler from the imblearn library\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Use RandomOverSampler to resample the datase using random_state=1\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test the distribution once again with `Counter`. The new result of `Counter({1: 1194, 0: 1194})` shows the data is now balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Counter again to verify imbalance removed\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "#### 1. The first cells in this section provide an example of how to fit and train your model using the `LogisticRegression` model from sklearn:\n",
    "* Import select model.\n",
    "* Instantiate model object.\n",
    "* Fit the model to the resampled data - `X_resampled` and `y_resampled`.\n",
    "* Predict the model using `X_test`.\n",
    "* Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification_report from sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression model and train it on the X_resampled data we created before\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = log_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report toevaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use the same approach as above to train and test the following ML Algorithms:\n",
    "* [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "* [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "* [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier model and train it on the X_resampled data we created before\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = rfc_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a GradientBoostingClassifier model and train it on the X_resampled data we created before\n",
    "gbc_model = GradientBoostingClassifier()\n",
    "gbc_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = gbc_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create a AdaBoostClassifier model and train it on the X_resampled data we created before\n",
    "abc_model = AdaBoostClassifier()\n",
    "abc_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = abc_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier from sklearn\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a XGBClassifier model and train it on the X_resampled data we created before\n",
    "xgbc_model = XGBClassifier()\n",
    "xgbc_model.fit(X_resampled, y_resampled)  \n",
    "\n",
    "# Use the model you trained to predict using X_test\n",
    "y_pred = xgbc_model.predict(X_test)   \n",
    "\n",
    "# Print out a classification report to evaluate performance\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using the classification report for each model, choose the model with the highest precision for use in your algo-trading program.\n",
    "#### 2. Save the selected model with the `joblib` libary to avoid retraining every time you wish to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the joblib library \n",
    "import joblib\n",
    "\n",
    "# Use the library to save the model that you want to use for trading\n",
    "joblib.dump(log_model, 'log_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement the strongest model using Apaca API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use the provided code to ping the Alpaca API and create the DataFrame needed to feed data into the model.\n",
    "   * This code will also store the correct feature data in `X` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tickers\n",
    "ticker_list = ['FB','AMZN','AAPL','NFLX', 'GOOGL', 'MSFT', 'TSLA']\n",
    "\n",
    "# Define Dates\n",
    "beg_date = '2021-01-06'\n",
    "end_date = '2021-01-06'\n",
    "\n",
    "# Convert the date in a format the Alpaca API reqires\n",
    "start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "end   =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=15, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "timeframe='1Min'\n",
    "\n",
    "# Use iloc to get the last 10 mins every time we pull new data\n",
    "prices = api.get_barset(ticker_list, \"minute\", start=start, end=end).df.iloc[-11:]\n",
    "prices.ffill(inplace=True)   \n",
    "\n",
    "# Create an empty DataFrame for closing prices\n",
    "df_closing_prices = pd.DataFrame()\n",
    "\n",
    "# Fetch the closing prices of our tickers\n",
    "df_closing_prices[\"FB\"] = prices[\"FB\"][\"close\"]\n",
    "df_closing_prices[\"AMZN\"] = prices[\"AMZN\"][\"close\"]\n",
    "df_closing_prices[\"AAPL\"] = prices[\"AAPL\"][\"close\"]\n",
    "df_closing_prices[\"NFLX\"] = prices[\"NFLX\"][\"close\"]\n",
    "df_closing_prices[\"GOOGL\"] = prices[\"GOOGL\"][\"close\"]\n",
    "df_closing_prices['MSFT'] = prices['MSFT'][\"close\"]\n",
    "df_closing_prices['TSLA'] = prices['TSLA'][\"close\"]\n",
    "\n",
    "print(df_closing_prices.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of momentums\n",
    "list_of_momentums = [1,5,10]\n",
    "\n",
    "for i in list_of_momentums:  \n",
    "    # Compute percentage change for each one of the momentums in the momentum list\n",
    "    returns_temp = df_closing_prices.pct_change(i)\n",
    "    # Unstack the returns \n",
    "    returns_temp = pd.DataFrame(returns_temp.unstack())\n",
    "    name = f'{i}_m_returns'\n",
    "    returns_temp.rename(columns={0: name}, inplace = True)\n",
    "    # Reset the index so we can merge based on index\n",
    "    returns_temp.reset_index(inplace = True)\n",
    "    # Merge newly computed returns with previously created returns\n",
    "    if i ==1:\n",
    "        returns = returns_temp\n",
    "    else:\n",
    "        returns = pd.merge(returns,returns_temp,left_on=['level_0', 'time'],right_on=['level_0', 'time'], how='left', suffixes=('_original', 'right'))\n",
    "\n",
    "# Drop nulls and set index\n",
    "returns.dropna(axis=0, how='any', inplace=True)\n",
    "returns.set_index(['level_0', 'time'], inplace=True)\n",
    "\n",
    "# Generate feature data and preview first 10 rows.\n",
    "X = returns\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using `joblib`, load the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously trained and saved model using joblib\n",
    "model = joblib.load('log_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the model file to make predicttions:\n",
    "* Use `predict` on `X` and save this as `y_pred`.\n",
    "* Convert `y_pred` to a DataFrame, setting the index to the index of `X`.\n",
    "* Rename the column 0 to 'buy', be sure to set `inplace =True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model file to predict on X\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Convert y_pred to a dataframe, set the index to the index of X\n",
    "y_df = pd.DataFrame(y_pred, index=X.index)\n",
    "\n",
    "# Rename the column 0 to 'buy', be sure to set inplace =True\n",
    "y_df.rename(columns={0: \"buy\"}, inplace = True)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter the stocks where 'buy' is equal to 1, saving the filter as `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the stocks where 'buy' is equal to 1\n",
    "y_pred = y_df.loc[y_df[\"buy\"] == 1]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Using the `y_pred` filter, create a dictionary called `buy_dict` and assign 'n' to each Ticker (key value) as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary from y_pred and assign a 'n' to each of them for now as a placeholder.\n",
    "buy_dict = dict.fromkeys(y_pred.index.get_level_values(0), 'n')\n",
    "buy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Obtain the total available equity in your account from the Alpaca API and store in a variable called `total_capital`. You will split the capital equally between all selected stocks per the CIO's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the total available equity in our account from the  Alpaca API\n",
    "account = api.get_account()\n",
    "total_capital = float(account.equity)\n",
    "print(f\"Total available capital: {total_capital}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute capital per stock, divide equity in account by number of stocks\n",
    "# Use Alpaca API to pull the equity in the account\n",
    "if len(buy_dict) > 0:\n",
    "    capital_per_stock = float(total_capital)/ len(buy_dict)\n",
    "else:\n",
    "    capital_per_stock = 0\n",
    "print(f'Capital per stock: {capital_per_stock}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Use a for-loop to iterate through `buy_dict` to determine the number stocks you need to buy for each ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for loop to iterate through dictionary of buys \n",
    "# Determine the number stocks we need to buy for each ticker\n",
    "for ticker in buy_dict:\n",
    "    try:\n",
    "        buy_dict[ticker] = int(capital_per_stock /int(prices[ticker].iloc[-1]['close']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(buy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Cancel all previous orders in the Alpaca API (so you don't buy more than intended) and sell all currently held stocks to close all positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel all previous orders in the Alpaca API\n",
    "api.cancel_all_orders()\n",
    "\n",
    "# Sell all currently held stocks to close all positions\n",
    "api.close_all_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Iterate through `buy_dict` and send a buy order for each ticker with their corresponding number of shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the buy_dict object and send a buy order for each ticker with a corresponding number of shares:\n",
    "\n",
    "for stock, qty in buy_dict.items():    \n",
    "    # Submit a market order to buy shares as described in buy_dict\n",
    "    api.submit_order(\n",
    "        symbol=stock,\n",
    "        qty=qty,\n",
    "        side='buy',\n",
    "        type='market',\n",
    "        time_in_force='gtc',\n",
    "    )\n",
    "    print(f'buying {stock} numShares {qty}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Make a function called `trade()` that incorporates all of the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all of the steps conducted above into the function trade\n",
    "def trade():\n",
    "\n",
    "    ticker_list = ['FB','AMZN','AAPL','NFLX', 'GOOGL', 'MSFT', 'TSLA']\n",
    "    # Notice that we remove the start and end variables since we want the latest prices.\n",
    "    timeframe='1Min'\n",
    "    # Use iloc to get the last 10 mins every time we pull new data\n",
    "    prices = api.get_barset(ticker_list, \"minute\").df.iloc[-11:]\n",
    "    prices.ffill(inplace=True)   \n",
    "\n",
    "    # Create and empty DataFrame for closing prices\n",
    "    df_closing_prices = pd.DataFrame()\n",
    "\n",
    "    # Fetch the closing prices of our tickers\n",
    "    df_closing_prices[\"FB\"] = prices[\"FB\"][\"close\"]\n",
    "    df_closing_prices[\"AMZN\"] = prices[\"AMZN\"][\"close\"]\n",
    "    df_closing_prices[\"AAPL\"] = prices[\"AAPL\"][\"close\"]\n",
    "    df_closing_prices[\"NFLX\"] = prices[\"NFLX\"][\"close\"]\n",
    "    df_closing_prices[\"GOOGL\"] = prices[\"GOOGL\"][\"close\"]\n",
    "    df_closing_prices['MSFT'] = prices['MSFT'][\"close\"]\n",
    "    df_closing_prices['TSLA'] = prices['TSLA'][\"close\"]\n",
    "    print(df_closing_prices.head())\n",
    "    \n",
    "    # Loop through momentums to build new DataFrame\n",
    "    list_of_momentums = [1,5,10]\n",
    "    for i in list_of_momentums:   \n",
    "        returns_temp = df_closing_prices.pct_change(i)\n",
    "        returns_temp = pd.DataFrame(returns_temp.unstack())\n",
    "        name = f'{i}_m_returns'\n",
    "        returns_temp.rename(columns={0: name}, inplace = True)\n",
    "        returns_temp.reset_index(inplace = True)\n",
    "        if i ==1:\n",
    "            returns = returns_temp\n",
    "        else:\n",
    "            returns = pd.merge(returns,returns_temp,left_on=['level_0', 'time'],right_on=['level_0', 'time'], how='left', suffixes=('_original', 'right'))\n",
    "\n",
    "    # Drop nulls and set index            \n",
    "    returns.dropna(axis=0, how='any', inplace=True)\n",
    "    returns.set_index(['level_0', 'time'], inplace=True)\n",
    "\n",
    "    # Preprocess data for model\n",
    "    model = joblib.load('log_model.pkl')\n",
    "    y_pred = model.predict(X)\n",
    "    y_df = pd.DataFrame(y_pred, index=X.index)\n",
    "    y_df.rename(columns={0: \"buy\"}, inplace = True)\n",
    "    y_pred = y_df.loc[y_df[\"buy\"] == 1]\n",
    "    \n",
    "    # Create the `buy_dict` object\n",
    "    buy_dict = dict.fromkeys(y_pred.index.get_level_values(0), 'n')\n",
    "    \n",
    "    # Split capital between stocks and determine buy or sell\n",
    "    account = api.get_account()\n",
    "    total_capital = float(account.equity)\n",
    "    if len(buy_dict) > 0:\n",
    "        capital_per_stock = float(total_capital)/ len(buy_dict)\n",
    "    else:\n",
    "        capital_per_stock = 0\n",
    "    for ticker in buy_dict:\n",
    "        try:\n",
    "            buy_dict[ticker] = int(capital_per_stock /int(prices[ticker].iloc[-1]['close']))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # Cancel pending orders and close positions\n",
    "    api.cancel_all_orders()\n",
    "    api.close_all_positions()\n",
    "    \n",
    "    # Submit orders\n",
    "    for stock, qty in buy_dict.items():    \n",
    "        # Submit a market order to buy shares as described in buy_dict\n",
    "        api.submit_order(\n",
    "            symbol=stock,\n",
    "            qty=qty,\n",
    "            side='buy',\n",
    "            type='market',\n",
    "            time_in_force='gtc',\n",
    "        )\n",
    "    print(f'buying {stock} numShares {qty}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import Python's schedule module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python's schedule module \n",
    "# These imports have been completed at the start of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the \"schedule\" module to automate the algorithm:\n",
    "* Clear the schedule with `.clear()`.\n",
    "* Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. `10:31:05`).\n",
    "* Use the Alpaca API to check whether the market is open.\n",
    "* Use run_pending() function inside schedule to execute the schedule you defined while the market is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The market is closed the next open market day will be 2022-02-25 09:30:00-05:00\n"
     ]
    }
   ],
   "source": [
    "# Clear the schedule\n",
    "schedule.clear()\n",
    "\n",
    "# Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. 10:31:05)\n",
    "trade_schedule = schedule.every().minute.at(\":05\").do(trade)\n",
    "\n",
    "# Use the Alpaca API to check whether the market is open\n",
    "clock = api.get_clock()\n",
    "\n",
    "# Use run_pending() function inside schedule to execute the schedule you defined as long as the market is open\n",
    "while clock.is_open == True:\n",
    "    print(f'The market trading widow for {clock.next_open} is open, executing trade function')\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(f'The market is closed the next open market day will be {clock.next_open}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Every 1 minute at 00:00:05 do trade() (last run: [never], next run: 2022-02-25 16:05:05)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Scheduled Jobs\n",
    "schedule.get_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
